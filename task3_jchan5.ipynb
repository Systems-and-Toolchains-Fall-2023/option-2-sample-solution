{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce17439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "col_names = ['tcp_flags','tcp_time_delta','tcp_len','mqtt_conack_flags','mqtt_conack_flags_reserved',\n",
    "             'mqtt_conack_flags_sp','mqtt_conack_val','mqtt_conflag_cleansess','mqtt_conflag_passwd',\n",
    "             'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_uname',\n",
    "             'mqtt_conflag_willflag','mqtt_conflags','mqtt_dupflag','mqtt_hdrflags','mqtt_kalive',\n",
    "             'mqtt_len','mqtt_msg','mqtt_msgid','mqtt_msgtype','mqtt_proto_len','mqtt_protoname',\n",
    "             'mqtt_qos','mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_ver','mqtt_willmsg',\n",
    "             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len','target']\n",
    "\n",
    "nominal_cols = ['tcp_flags', 'tcp_time_delta', 'tcp_len', 'mqtt_msg', 'mqtt_protoname',\\\n",
    "                'mqtt_willmsg', 'mqtt_willtopic', 'target']\n",
    "\n",
    "binary_cols = ['mqtt_conack_flags_reserved', 'mqtt_conack_flags_sp', 'mqtt_conflag_cleansess', \\\n",
    "               'mqtt_conflag_passwd', 'mqtt_conflag_reserved', 'mqtt_conflag_retain', 'mqtt_conflag_uname',\\\n",
    "               'mqtt_conflag_willflag', 'mqtt_dupflag', 'mqtt_retain']\n",
    "\n",
    "continuous_cols = ['mqtt_conack_val', 'mqtt_conack_flags', 'mqtt_conflag_qos', 'mqtt_conflags', 'mqtt_hdrflags', 'mqtt_kalive',\\\n",
    "                 'mqtt_len', 'mqtt_msgid', 'mqtt_msgtype', 'mqtt_proto_len', 'mqtt_qos',  'mqtt_sub_qos',\\\n",
    "                  'mqtt_suback_qos', 'mqtt_ver', 'mqtt_willmsg_len', 'mqtt_willtopic_len']\n",
    "\n",
    "corelated_cols_to_remove = []\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        label_to_binary = udf(lambda name: 0.0 if name == 'slowite'\\\n",
    "                              else 2.0 if name == 'bruteforce'\\\n",
    "                              else 3.0 if name == 'flood'\\\n",
    "                              else 4.0 if name == 'malformed'\\\n",
    "                              else 5.0 if name == 'dos'\\\n",
    "                              else 5.0\n",
    "                             )\n",
    "        output_df = dataset.withColumn('outcome', label_to_binary(col('target'))).drop(\"target\")  \n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        #output_df = output_df.drop('difficulty')\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    \n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\") # handleInvalid = 'keep'\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n",
    "    return pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d4578fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_flags: string (nullable = true)\n",
      " |-- tcp_time_delta: double (nullable = true)\n",
      " |-- tcp_len: integer (nullable = true)\n",
      " |-- mqtt_conack_flags: integer (nullable = true)\n",
      " |-- mqtt_conack_flags_reserved: boolean (nullable = true)\n",
      " |-- mqtt_conack_flags_sp: boolean (nullable = true)\n",
      " |-- mqtt_conack_val: integer (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: boolean (nullable = true)\n",
      " |-- mqtt_conflag_passwd: boolean (nullable = true)\n",
      " |-- mqtt_conflag_qos: integer (nullable = true)\n",
      " |-- mqtt_conflag_reserved: boolean (nullable = true)\n",
      " |-- mqtt_conflag_retain: boolean (nullable = true)\n",
      " |-- mqtt_conflag_uname: boolean (nullable = true)\n",
      " |-- mqtt_conflag_willflag: boolean (nullable = true)\n",
      " |-- mqtt_conflags: integer (nullable = true)\n",
      " |-- mqtt_dupflag: boolean (nullable = true)\n",
      " |-- mqtt_hdrflags: integer (nullable = true)\n",
      " |-- mqtt_kalive: integer (nullable = true)\n",
      " |-- mqtt_len: integer (nullable = true)\n",
      " |-- mqtt_msg: string (nullable = true)\n",
      " |-- mqtt_msgid: integer (nullable = true)\n",
      " |-- mqtt_msgtype: integer (nullable = true)\n",
      " |-- mqtt_proto_len: integer (nullable = true)\n",
      " |-- mqtt_protoname: string (nullable = true)\n",
      " |-- mqtt_qos: integer (nullable = true)\n",
      " |-- mqtt_retain: boolean (nullable = true)\n",
      " |-- mqtt_sub_qos: integer (nullable = true)\n",
      " |-- mqtt_suback_qos: integer (nullable = true)\n",
      " |-- mqtt_ver: integer (nullable = true)\n",
      " |-- mqtt_willmsg: string (nullable = true)\n",
      " |-- mqtt_willmsg_len: integer (nullable = true)\n",
      " |-- mqtt_willtopic: string (nullable = true)\n",
      " |-- mqtt_willtopic_len: integer (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you installed Spark on windows, \n",
    "# you may need findspark and need to initialize it prior to being able to use pyspark\n",
    "# Also, you may need to initialize SparkContext yourself.\n",
    "# Uncomment the following lines if you are using Windows!\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SystemsToolChains\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "mqtt_schema = StructType([\n",
    "    StructField(\"tcp_flags\", StringType(), True), \n",
    "    StructField(\"tcp_time_delta\", DoubleType(), True),\n",
    "    StructField(\"tcp_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conack_flags\", IntegerType(), True), \n",
    "    StructField(\"mqtt_conack_flags_reserved\", BooleanType(), True),\n",
    "    StructField(\"mqtt_conack_flags_sp\", BooleanType(), True), \n",
    "    StructField(\"mqtt_conack_val\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_cleansess\", BooleanType(), True),\n",
    "    StructField(\"mqtt_conflag_passwd\", BooleanType(), True),\n",
    "    StructField(\"mqtt_conflag_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_conflag_reserved\", BooleanType(), True),\n",
    "    StructField(\"mqtt_conflag_retain\", BooleanType(), True),\n",
    "    StructField(\"mqtt_conflag_uname\", BooleanType(), True),\n",
    "    StructField(\"mqtt_conflag_willflag\", BooleanType(), True),\n",
    "    StructField(\"mqtt_conflags\", IntegerType(), True),\n",
    "    StructField(\"mqtt_dupflag\", BooleanType(), True),\n",
    "    StructField(\"mqtt_hdrflags\", IntegerType(), True),\n",
    "    StructField(\"mqtt_kalive\", IntegerType(), True),\n",
    "    StructField(\"mqtt_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_msg\", StringType(), True),\n",
    "    StructField(\"mqtt_msgid\", IntegerType(), True),\n",
    "    StructField(\"mqtt_msgtype\", IntegerType(), True),\n",
    "    StructField(\"mqtt_proto_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_protoname\", StringType(), True),\n",
    "    StructField(\"mqtt_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_retain\", BooleanType(), True),\n",
    "    StructField(\"mqtt_sub_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_suback_qos\", IntegerType(), True),\n",
    "    StructField(\"mqtt_ver\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willmsg\", StringType(), True),\n",
    "    StructField(\"mqtt_willmsg_len\", IntegerType(), True),\n",
    "    StructField(\"mqtt_willtopic\", StringType(), True),\n",
    "    StructField(\"mqtt_willtopic_len\", IntegerType(), True),\n",
    "    StructField(\"target\", StringType(), True)\n",
    "])\n",
    "\n",
    "mqtt_raw = spark.read.csv('data/train70_reduced.csv',header=True, schema=mqtt_schema).toDF(*col_names)\n",
    "mqtt_test_raw = spark.read.csv('data/test30_reduced.csv',header=True, inferSchema=True).toDF(*col_names)\n",
    "\n",
    "mqtt_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b11394d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2512572328.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[39], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    mqtt_schema = = StructType([\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Column Rename\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType\n",
    "# mqtt_conack_flags | int\n",
    "# mqtt_conack_flags_reserved | boolean\n",
    "# mqtt_conack_flags_sp | boolean\n",
    "# mqtt_conack_val | int\n",
    "\n",
    "\n",
    "renamed_columns_df = df.withColumnRenamed(\"rerror_rate\",\"newly_renamed_rerror_rate\")\n",
    "# Casting one column type\n",
    "casted_types_df = (renamed_columns_df.withColumn(\"new_column_srv_serror_rate\", \\\n",
    "                    renamed_columns_df[\"srv_serror_rate\"] \\\n",
    "                    .cast(\"integer\")).drop(\"srv_serror_rate\")\n",
    "                    .distinct() # deleting duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(mqtt_raw)\n",
    "\n",
    "mqtt_df = preprocess_pipeline_model.transform(mqtt_raw)\n",
    "mqtt_df_test = preprocess_pipeline_model.transform(mqtt_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ddc8014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|outcome|\n",
      "+--------------------+-------+\n",
      "|(45418,[0,1,2,3,4...|    5.0|\n",
      "|(45418,[3,5,7,25,...|    5.0|\n",
      "|(45418,[3,5,6,7,9...|    5.0|\n",
      "|(45418,[3,5,6,7,9...|    5.0|\n",
      "|(45418,[3,5,7,25,...|    5.0|\n",
      "|(45418,[3,5,6,7,2...|    5.0|\n",
      "|(45418,[26,41,898...|    5.0|\n",
      "|(45418,[26,37,898...|    5.0|\n",
      "|(45418,[3,5,7,25,...|    5.0|\n",
      "|(45418,[3,5,7,25,...|    5.0|\n",
      "|(45418,[3,5,6,7,2...|    5.0|\n",
      "|(45418,[3,5,6,7,9...|    5.0|\n",
      "|(45418,[30,34,898...|    4.0|\n",
      "|(45418,[3,5,7,25,...|    5.0|\n",
      "|(45418,[3,5,7,25,...|    5.0|\n",
      "|(45418,[0,3,5,7,2...|    2.0|\n",
      "|(45418,[3,5,6,7,2...|    5.0|\n",
      "|(45418,[3,5,6,7,2...|    5.0|\n",
      "|(45418,[26,877,89...|    2.0|\n",
      "|(45418,[26,33,898...|    5.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- outcome: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mqtt_df.show()\n",
    "mqtt_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17495934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " tcp_flags                  | 0x00000018 \n",
      " tcp_time_delta             | 0.998867   \n",
      " tcp_len                    | 10         \n",
      " mqtt_conack_flags          | 0          \n",
      " mqtt_conack_flags_reserved | 0.0        \n",
      " mqtt_conack_flags_sp       | 0.0        \n",
      " mqtt_conack_val            | 0.0        \n",
      " mqtt_conflag_cleansess     | 0.0        \n",
      " mqtt_conflag_passwd        | 0.0        \n",
      " mqtt_conflag_qos           | 0.0        \n",
      " mqtt_conflag_reserved      | 0.0        \n",
      " mqtt_conflag_retain        | 0.0        \n",
      " mqtt_conflag_uname         | 0.0        \n",
      " mqtt_conflag_willflag      | 0.0        \n",
      " mqtt_conflags              | 0          \n",
      " mqtt_dupflag               | 0.0        \n",
      " mqtt_hdrflags              | 0x00000030 \n",
      " mqtt_kalive                | 0.0        \n",
      " mqtt_len                   | 8.0        \n",
      " mqtt_msg                   | 32         \n",
      " mqtt_msgid                 | 0.0        \n",
      " mqtt_msgtype               | 3.0        \n",
      " mqtt_proto_len             | 0.0        \n",
      " mqtt_protoname             | 0          \n",
      " mqtt_qos                   | 0.0        \n",
      " mqtt_retain                | 0.0        \n",
      " mqtt_sub_qos               | 0.0        \n",
      " mqtt_suback_qos            | 0.0        \n",
      " mqtt_ver                   | 0.0        \n",
      " mqtt_willmsg               | 0.0        \n",
      " mqtt_willmsg_len           | 0.0        \n",
      " mqtt_willtopic             | 0.0        \n",
      " mqtt_willtopic_len         | 0.0        \n",
      " target                     | legitimate \n",
      " Original_Dataset           | train      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Read the first dataset and add a constant column with label \"test\"\n",
    "df_train = spark.read.csv(\"data/train70_reduced.csv\", header=True, inferSchema=True).toDF(*col_names)\n",
    "df_train = df_train.withColumn(\"Original_Dataset\", lit(\"train\"))\n",
    "\n",
    "# Read the second dataset and add a constant column with label \"train\"\n",
    "df_test = spark.read.csv(\"data/test30_reduced.csv\", header=True, inferSchema=True).toDF(*col_names)\n",
    "df_test = df_test.withColumn(\"Original_Dataset\", lit(\"test\"))\n",
    "\n",
    "\n",
    "merged_df = df_train.union(df_test)\n",
    "merged_df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "483b55bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joncc\\Documents\\GitHub\\course-project-option-2-TaxiCabNumber-camerril\\task3_jchan5.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m numeric_features \u001b[39m=\u001b[39m [feature[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m df_train \u001b[39mif\u001b[39;00m feature[\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m continuous_cols]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(numeric_features)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\joncc\\Documents\\GitHub\\course-project-option-2-TaxiCabNumber-camerril\\task3_jchan5.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m numeric_features \u001b[39m=\u001b[39m [feature[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m df_train \u001b[39mif\u001b[39;00m feature[\u001b[39m1\u001b[39;49m] \u001b[39min\u001b[39;49;00m continuous_cols]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(numeric_features)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\column.py:1369\u001b[0m, in \u001b[0;36mColumn.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__nonzero__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1369\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1370\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot convert column into bool: please use \u001b[39m\u001b[39m'\u001b[39m\u001b[39m&\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for \u001b[39m\u001b[39m'\u001b[39m\u001b[39mand\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for \u001b[39m\u001b[39m'\u001b[39m\u001b[39mor\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m~\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot\u001b[39m\u001b[39m'\u001b[39m\u001b[39m when building DataFrame boolean expressions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "numeric_features = [feature[0] for feature in df_train if feature[1] in continuous_cols]\n",
    "print(numeric_features)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6d783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         outcome\n",
      "outcome      1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joncc\\AppData\\Local\\Temp\\ipykernel_70960\\3326994836.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  correlation_matrix = mqtt_df.toPandas().corr()\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = mqtt_df.toPandas().corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410e72f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2816.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 30.0 failed 1 times, most recent failure: Lost task 2.0 in stage 30.0 (TID 83) (LAPTOP-R6MIHO76.wifi.local.cmu.edu executor driver): java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (45418,[3,5,6,7,9,25,35,9070,9711,43756,45408,45410,45411,45413],[NaN,NaN,NaN,NaN,NaN,2.012857332503923,3.533718283042206,39.57494549165696,10.271119772922047,481.29720547703164,10.268826489694721,481.29720547703164,481.29720547703164,2.0469576662564344])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1172)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1166)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1259)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1226)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1212)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1212)\r\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (45418,[3,5,6,7,9,25,35,9070,9711,43756,45408,45410,45411,45413],[NaN,NaN,NaN,NaN,NaN,2.012857332503923,3.533718283042206,39.57494549165696,10.271119772922047,481.29720547703164,10.268826489694721,481.29720547703164,481.29720547703164,2.0469576662564344])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joncc\\Documents\\GitHub\\course-project-option-2-TaxiCabNumber-camerril\\task3_jchan5.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lr \u001b[39m=\u001b[39m LogisticRegression(featuresCol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m, labelCol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutcome\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joncc/Documents/GitHub/course-project-option-2-TaxiCabNumber-camerril/task3_jchan5.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lrModel \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39;49mfit(mqtt_df)\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2816.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 30.0 failed 1 times, most recent failure: Lost task 2.0 in stage 30.0 (TID 83) (LAPTOP-R6MIHO76.wifi.local.cmu.edu executor driver): java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (45418,[3,5,6,7,9,25,35,9070,9711,43756,45408,45410,45411,45413],[NaN,NaN,NaN,NaN,NaN,2.012857332503923,3.533718283042206,39.57494549165696,10.271119772922047,481.29720547703164,10.268826489694721,481.29720547703164,481.29720547703164,2.0469576662564344])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1172)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1166)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1259)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1226)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1212)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1212)\r\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (45418,[3,5,6,7,9,25,35,9070,9711,43756,45408,45410,45411,45413],[NaN,NaN,NaN,NaN,NaN,2.012857332503923,3.533718283042206,39.57494549165696,10.271119772922047,481.29720547703164,10.268826489694721,481.29720547703164,481.29720547703164,2.0469576662564344])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "lrModel = lr.fit(mqtt_df) # fit the logistic regression model to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d635e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
